{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f71905",
   "metadata": {},
   "source": [
    "## Merge the scraped data (Excel Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e889c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 51 out of 51 files into Merged_Data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from zipfile import BadZipFile\n",
    "\n",
    "# Set the directory where your Excel files are located\n",
    "folder_path = '/Users/kittang/Desktop/Master Notes/SDSC 6014/Group Project Materials/Youtube Trending Project/DataSets/Final Raw Data'  # Replace with your folder path\n",
    "output_file_name = 'Merged_Data.xlsx'  # Name of the merged output file\n",
    "\n",
    "# List all Excel files in the folder\n",
    "excel_files = [f for f in os.listdir(folder_path) if f.endswith(('.xlsx', '.xls'))]\n",
    "\n",
    "# Initialize an empty list to hold dataframes\n",
    "frames = []\n",
    "\n",
    "# Loop through the files and read them into pandas dataframes\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    try:\n",
    "        # Determine the engine based on the file extension\n",
    "        engine = 'openpyxl' if file.endswith('.xlsx') else 'xlrd'\n",
    "        df = pd.read_excel(file_path, engine=engine)\n",
    "        \n",
    "        # Make sure 'Date' is a datetime type; replace 'Date' with your actual date column name\n",
    "        df['Date'] = pd.to_datetime(df['Trending_Date'])\n",
    "        \n",
    "        # Sort the dataframe by the date column\n",
    "        df.sort_values(by='Date', inplace=True)\n",
    "        \n",
    "        # Add a 'Trending Number' column that starts at 1 for each day\n",
    "        df['Trending Number'] = df.groupby('Date').cumcount() + 1\n",
    "        \n",
    "        frames.append(df)\n",
    "    except BadZipFile:\n",
    "        print(f\"Error reading {file}: File may be corrupt or not a .xlsx file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {file}: {e}\")\n",
    "\n",
    "# Concatenate all the dataframes into one merged dataframe\n",
    "merged_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Write the merged dataframe to a new Excel file\n",
    "merged_df.to_excel(output_file_name, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Merged {len(frames)} out of {len(excel_files)} files into {output_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f46db2",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0a75d",
   "metadata": {},
   "source": [
    "### Removed column with same value in all / most rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4730345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trending_Date</th>\n",
       "      <th>Category_ID</th>\n",
       "      <th>View_Number</th>\n",
       "      <th>Like_Number</th>\n",
       "      <th>Comment_Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Trending Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6715</td>\n",
       "      <td>6715.000000</td>\n",
       "      <td>6.715000e+03</td>\n",
       "      <td>6.693000e+03</td>\n",
       "      <td>6714.000000</td>\n",
       "      <td>6715</td>\n",
       "      <td>6715.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2024-03-06 07:17:28.101265664</td>\n",
       "      <td>20.353239</td>\n",
       "      <td>1.580309e+07</td>\n",
       "      <td>4.849373e+05</td>\n",
       "      <td>7748.944296</td>\n",
       "      <td>2024-03-06 07:17:28.101265664</td>\n",
       "      <td>66.655249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2024-02-10 00:00:00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.844700e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-02-10 00:00:00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2024-02-22 00:00:00</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.520775e+05</td>\n",
       "      <td>9.194000e+03</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>2024-02-22 00:00:00</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2024-03-06 00:00:00</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.091073e+06</td>\n",
       "      <td>3.248800e+04</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>2024-03-06 00:00:00</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2024-03-20 00:00:00</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.574705e+07</td>\n",
       "      <td>4.021470e+05</td>\n",
       "      <td>3617.000000</td>\n",
       "      <td>2024-03-20 00:00:00</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-03-31 00:00:00</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2.384422e+08</td>\n",
       "      <td>8.864461e+06</td>\n",
       "      <td>300174.000000</td>\n",
       "      <td>2024-03-31 00:00:00</td>\n",
       "      <td>151.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.933026</td>\n",
       "      <td>3.256622e+07</td>\n",
       "      <td>1.060866e+06</td>\n",
       "      <td>26616.044783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.570991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Trending_Date  Category_ID   View_Number   Like_Number  \\\n",
       "count                           6715  6715.000000  6.715000e+03  6.693000e+03   \n",
       "mean   2024-03-06 07:17:28.101265664    20.353239  1.580309e+07  4.849373e+05   \n",
       "min              2024-02-10 00:00:00     1.000000  3.844700e+04  0.000000e+00   \n",
       "25%              2024-02-22 00:00:00    17.000000  3.520775e+05  9.194000e+03   \n",
       "50%              2024-03-06 00:00:00    22.000000  1.091073e+06  3.248800e+04   \n",
       "75%              2024-03-20 00:00:00    24.000000  1.574705e+07  4.021470e+05   \n",
       "max              2024-03-31 00:00:00    29.000000  2.384422e+08  8.864461e+06   \n",
       "std                              NaN     5.933026  3.256622e+07  1.060866e+06   \n",
       "\n",
       "       Comment_Number                           Date  Trending Number  \n",
       "count     6714.000000                           6715      6715.000000  \n",
       "mean      7748.944296  2024-03-06 07:17:28.101265664        66.655249  \n",
       "min          0.000000            2024-02-10 00:00:00         1.000000  \n",
       "25%        181.000000            2024-02-22 00:00:00        33.000000  \n",
       "50%        512.000000            2024-03-06 00:00:00        66.000000  \n",
       "75%       3617.000000            2024-03-20 00:00:00        99.000000  \n",
       "max     300174.000000            2024-03-31 00:00:00       151.000000  \n",
       "std      26616.044783                            NaN        38.570991  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.drop(columns=['HD','Disikes_Enabled', 'Dislike_Number','Fav_Number','Likes_Enabled','Disikes_Enabled'])\n",
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb0958",
   "metadata": {},
   "source": [
    "### Removed rows with no like / comment number for an organized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22cde13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trending_Date</th>\n",
       "      <th>Category_ID</th>\n",
       "      <th>View_Number</th>\n",
       "      <th>Like_Number</th>\n",
       "      <th>Comment_Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Trending Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6692</td>\n",
       "      <td>6692.000000</td>\n",
       "      <td>6.692000e+03</td>\n",
       "      <td>6.692000e+03</td>\n",
       "      <td>6692.000000</td>\n",
       "      <td>6692</td>\n",
       "      <td>6692.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2024-03-06 06:47:33.317393920</td>\n",
       "      <td>20.359534</td>\n",
       "      <td>1.580945e+07</td>\n",
       "      <td>4.849872e+05</td>\n",
       "      <td>7765.678273</td>\n",
       "      <td>2024-03-06 06:47:33.317393920</td>\n",
       "      <td>66.605200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2024-02-10 00:00:00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.844700e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-02-10 00:00:00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2024-02-22 00:00:00</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.511815e+05</td>\n",
       "      <td>9.193000e+03</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>2024-02-22 00:00:00</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2024-03-06 00:00:00</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.082094e+06</td>\n",
       "      <td>3.247650e+04</td>\n",
       "      <td>507.500000</td>\n",
       "      <td>2024-03-06 00:00:00</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2024-03-20 00:00:00</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.564320e+07</td>\n",
       "      <td>4.025708e+05</td>\n",
       "      <td>3626.500000</td>\n",
       "      <td>2024-03-20 00:00:00</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-03-31 00:00:00</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2.384422e+08</td>\n",
       "      <td>8.864461e+06</td>\n",
       "      <td>300174.000000</td>\n",
       "      <td>2024-03-31 00:00:00</td>\n",
       "      <td>151.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.939829</td>\n",
       "      <td>3.262045e+07</td>\n",
       "      <td>1.060937e+06</td>\n",
       "      <td>26657.911881</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.522381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Trending_Date  Category_ID   View_Number   Like_Number  \\\n",
       "count                           6692  6692.000000  6.692000e+03  6.692000e+03   \n",
       "mean   2024-03-06 06:47:33.317393920    20.359534  1.580945e+07  4.849872e+05   \n",
       "min              2024-02-10 00:00:00     1.000000  3.844700e+04  0.000000e+00   \n",
       "25%              2024-02-22 00:00:00    17.000000  3.511815e+05  9.193000e+03   \n",
       "50%              2024-03-06 00:00:00    22.000000  1.082094e+06  3.247650e+04   \n",
       "75%              2024-03-20 00:00:00    24.000000  1.564320e+07  4.025708e+05   \n",
       "max              2024-03-31 00:00:00    29.000000  2.384422e+08  8.864461e+06   \n",
       "std                              NaN     5.939829  3.262045e+07  1.060937e+06   \n",
       "\n",
       "       Comment_Number                           Date  Trending Number  \n",
       "count     6692.000000                           6692      6692.000000  \n",
       "mean      7765.678273  2024-03-06 06:47:33.317393920        66.605200  \n",
       "min          0.000000            2024-02-10 00:00:00         1.000000  \n",
       "25%        181.000000            2024-02-22 00:00:00        33.000000  \n",
       "50%        507.500000            2024-03-06 00:00:00        66.000000  \n",
       "75%       3626.500000            2024-03-20 00:00:00        99.000000  \n",
       "max     300174.000000            2024-03-31 00:00:00       151.000000  \n",
       "std      26657.911881                            NaN        38.522381  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df[merged_df['Like_Number'].notna()]\n",
    "merged_df = merged_df[merged_df['Comment_Number'].notna()]\n",
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c73932",
   "metadata": {},
   "source": [
    "### Turn duration into seconds for EDA purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a768e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['DurationinSec'] = pd.to_datetime(merged_df['Duration'], format=\"%H:%M:%S\", errors=\"coerce\").fillna(pd.to_datetime(merged_df['Duration'], format=\"%M:%S\", errors=\"coerce\"))\n",
    "merged_df['DurationinSec'] = (merged_df['DurationinSec'] - pd.Timestamp('1900-01-01')) // pd.Timedelta(seconds=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff5e77",
   "metadata": {},
   "source": [
    "### Identified video with 60 sec or less than 60 sec as shorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4056bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df['DurationinSec'] <= 60, 'Shorts'] = 1\n",
    "merged_df.loc[merged_df['DurationinSec'] > 60, 'Shorts'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb544c4a",
   "metadata": {},
   "source": [
    "### For rumor about trending video's title:\n",
    "\n",
    "- Add tags in title\n",
    "- Add number in Title\n",
    "- Add emoji in title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "420c9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df['Video_Name'].str.contains(\"#\"), 'TagsInTitle'] = 1\n",
    "merged_df['TagsInTitle'] = merged_df['TagsInTitle'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5118f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df['Video_Name'].str.contains(r'\\d'), 'NumInTitle'] = 1\n",
    "merged_df['NumInTitle'] = merged_df['NumInTitle'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69e09bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df['Video_Name'].str.contains(r'[\\U00010000-\\U0010ffff]'), 'EmojiInTitle'] = 1\n",
    "merged_df['EmojiInTitle'] = merged_df['EmojiInTitle'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216124a1",
   "metadata": {},
   "source": [
    "### For checking whether it's a Chi or Eng Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de93cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df['Video_Name'].str.contains(r'[\\u4e00-\\u9fff\\u3400-\\u4dbf\\U00020000-\\U0002a6df\\U0002a700-\\U0002ebef\\U00030000-\\U000323af\\ufa0e\\ufa0f\\ufa11\\ufa13\\ufa14\\ufa1f\\ufa21\\ufa23\\ufa24\\ufa27\\ufa28\\ufa29\\u3006\\u3007][\\ufe00-\\ufe0f\\U000e0100-\\U000e01ef]?'), 'ChineseInTitle'] = 1\n",
    "merged_df['ChineseInTitle'] = merged_df['ChineseInTitle'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e747d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df['Video_Name'].str.contains(r' [A-z][A-z]+'), 'EnglishInTitle'] = 1\n",
    "merged_df['EnglishInTitle'] = merged_df['EnglishInTitle'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7e41828",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[(merged_df['ChineseInTitle'] == 1) & (merged_df['EnglishInTitle'] == 1), 'Eng&ChiInTitle'] = 1\n",
    "merged_df['Eng&ChiInTitle'] = merged_df['Eng&ChiInTitle'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dd67385",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[(merged_df['ChineseInTitle'] == 0) & (merged_df['EnglishInTitle'] == 0), 'OtherLanguageInTitle'] = 1\n",
    "merged_df['OtherLanguageInTitle'] = merged_df['OtherLanguageInTitle'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f20de0",
   "metadata": {},
   "source": [
    "### For checking whether it has Chi or Eng tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2fc46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df['Tags'].str.contains(r'[\\u4e00-\\u9fff\\u3400-\\u4dbf\\U00020000-\\U0002a6df\\U0002a700-\\U0002ebef\\U00030000-\\U000323af\\ufa0e\\ufa0f\\ufa11\\ufa13\\ufa14\\ufa1f\\ufa21\\ufa23\\ufa24\\ufa27\\ufa28\\ufa29\\u3006\\u3007][\\ufe00-\\ufe0f\\U000e0100-\\U000e01ef]?',na=False), 'ChineseInTags'] = 1\n",
    "merged_df['ChineseInTags'] = merged_df['ChineseInTags'].fillna(0)\n",
    "merged_df.loc[merged_df['Tags'].str.contains(r',[A-z][A-z]+',na=False), 'EnglishInTags'] = 1\n",
    "merged_df.loc[merged_df['Tags'].str.contains(r'[A-z][A-z]+,',na=False), 'EnglishInTags'] = 1\n",
    "merged_df['EnglishInTags'] = merged_df['EnglishInTags'].fillna(0)\n",
    "merged_df.loc[(merged_df['ChineseInTags'] == 1) & (merged_df['EnglishInTags'] == 1), 'Eng&ChiInTags'] = 1\n",
    "merged_df['Eng&ChiInTags'] = merged_df['Eng&ChiInTags'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2777e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Trending_Date                   Channel_Name          Publish Date  \\\n",
      "0       2024-02-26            “獨生子的日常”YouTube官方頻道  2024-02-21T11:00:30Z   \n",
      "1       2024-02-26  西川貴教 Official YouTube Channel  2024-02-09T09:00:24Z   \n",
      "2       2024-02-26                        MrBeast  2024-02-10T17:00:00Z   \n",
      "3       2024-02-26                          醒醒吧张律  2024-02-07T07:00:16Z   \n",
      "4       2024-02-26              Hooper Highlights  2024-02-18T03:13:41Z   \n",
      "...            ...                            ...                   ...   \n",
      "6710    2024-03-24                     城寨 Singjai  2024-03-23T16:14:01Z   \n",
      "6711    2024-03-24                     RFA 自由亞洲粵語  2024-03-22T23:00:06Z   \n",
      "6712    2024-03-24                           FASH  2024-03-19T12:00:10Z   \n",
      "6713    2024-03-24                          名侦探小宇  2024-03-10T10:00:27Z   \n",
      "6714    2024-03-24                 陳蕾Panther Chan  2024-02-22T13:32:27Z   \n",
      "\n",
      "                                             Video_Name  Category_ID  \\\n",
      "0                【獨生子的日常】小黄喝醉的后续来了#铲屎官的乐趣 #小奶猫 #有镜头感的小猫           15   \n",
      "1     西川貴教 with t.komuro - FREEDOM × 『機動戦士ガンダムSEED F...           10   \n",
      "2                Face Your Biggest Fear To Win $800,000           24   \n",
      "3                                 你和你的朋友、家人還剩下多少相處的時間呢？           22   \n",
      "4     Stephen Curry vs Sabrina Ionescu 3 Point Conte...           17   \n",
      "...                                                 ...          ...   \n",
      "6710  城寨國際：23 /3/2024 廿三條生效國際反應強烈 一國兩制信任全失 中共以為國際社會政...           25   \n",
      "6711              余茂春批評中國　將香港由寶地變成廢墟｜粵語新聞報道（03-22-2024）           25   \n",
      "6712  Miss Delight show something (Poppy Playtime 3 ...           20   \n",
      "6713  從窗外射進奇怪的光點，竟是求救訊號 #懸疑 #抖音短劇 #人工智慧  #兼职 #名偵探 #安...           22   \n",
      "6714  陳蕾 Panther Chan - 念 Light In Your Heart (Offic...           10   \n",
      "\n",
      "        Category_Name  View_Number  Like_Number  Comment_Number Duration  ...  \\\n",
      "0      Pets & Animals       992405      31557.0           160.0    00:24  ...   \n",
      "1               Music      5236117     106482.0          3486.0    05:19  ...   \n",
      "2       Entertainment    139534648    4673648.0        171720.0    22:03  ...   \n",
      "3      People & Blogs       960480      59396.0           250.0    01:00  ...   \n",
      "4              Sports      1232953      17089.0          1505.0    07:04  ...   \n",
      "...               ...          ...          ...             ...      ...  ...   \n",
      "6710  News & Politics        85076       7623.0           157.0  2:02:11  ...   \n",
      "6711  News & Politics       132651       5848.0           273.0    29:17  ...   \n",
      "6712           Gaming     16117645     726050.0         15957.0    00:11  ...   \n",
      "6713   People & Blogs       821466      43462.0           137.0    01:00  ...   \n",
      "6714            Music       464964       8100.0           344.0    04:49  ...   \n",
      "\n",
      "     TagsInTitle NumInTitle EmojiInTitle  ChineseInTitle  EnglishInTitle  \\\n",
      "0            1.0        0.0          0.0             1.0             0.0   \n",
      "1            0.0        0.0          0.0             1.0             1.0   \n",
      "2            0.0        1.0          0.0             0.0             1.0   \n",
      "3            0.0        0.0          0.0             1.0             0.0   \n",
      "4            0.0        1.0          0.0             0.0             1.0   \n",
      "...          ...        ...          ...             ...             ...   \n",
      "6710         0.0        1.0          0.0             1.0             0.0   \n",
      "6711         0.0        1.0          0.0             1.0             0.0   \n",
      "6712         0.0        1.0          0.0             0.0             1.0   \n",
      "6713         1.0        0.0          0.0             1.0             0.0   \n",
      "6714         0.0        0.0          0.0             1.0             1.0   \n",
      "\n",
      "      Eng&ChiInTitle  OtherLanguageInTitle  ChineseInTags  EnglishInTags  \\\n",
      "0                0.0                   0.0            1.0            1.0   \n",
      "1                1.0                   0.0            1.0            1.0   \n",
      "2                0.0                   0.0            0.0            0.0   \n",
      "3                0.0                   0.0            1.0            1.0   \n",
      "4                0.0                   0.0            0.0            1.0   \n",
      "...              ...                   ...            ...            ...   \n",
      "6710             0.0                   0.0            1.0            1.0   \n",
      "6711             0.0                   0.0            1.0            1.0   \n",
      "6712             0.0                   0.0            0.0            1.0   \n",
      "6713             0.0                   0.0            1.0            0.0   \n",
      "6714             1.0                   0.0            1.0            0.0   \n",
      "\n",
      "      Eng&ChiInTags  \n",
      "0               1.0  \n",
      "1               1.0  \n",
      "2               0.0  \n",
      "3               1.0  \n",
      "4               0.0  \n",
      "...             ...  \n",
      "6710            1.0  \n",
      "6711            1.0  \n",
      "6712            0.0  \n",
      "6713            0.0  \n",
      "6714            0.0  \n",
      "\n",
      "[6692 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb08d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_excel(\"cleanedDataset.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e7ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b905a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"cleanedDataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98b6ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exptags = df.copy()\n",
    "\n",
    "# Expanding the 'Tags' into separate rows, similar to our previous approach\n",
    "df_exptags = df_exptags.dropna(subset=['Tags']).assign(Expanded_Tags=df_exptags['Tags'].str.split(',')).explode('Expanded_Tags')\n",
    "\n",
    "# Resetting index for cleanliness\n",
    "df_exptags.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40b121d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exptags = df_exptags.loc[df_exptags.groupby('Video_Name')['Trending_Date'].idxmax()]\n",
    "df_exptags['Video_Name_Length'] = df_exptags['Video_Name'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc31230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_excel(\"cleanedDatasetR.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08f5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
